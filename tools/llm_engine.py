import os
import json
from typing import Optional, Any, List, Dict
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class MockLLM:
    """æ¨¡æ‹Ÿ LLM ç±»ï¼Œç”¨äºæµ‹è¯•æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”"""
    
    def __init__(self, model: str = "mock-deepseek", temperature: float = 0.7):
        self.model = model
        self.temperature = temperature
    
    def invoke(self, messages: List[Dict[str, str]]) -> Any:
        """è¿”å›æ¨¡æ‹Ÿå“åº”"""
        # ä»æ¶ˆæ¯ä¸­æå–å†…å®¹ï¼Œç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        user_message = ""
        for msg in messages:
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if "ç®€æŠ¥" in user_message or "brief" in user_message.lower():
            mock_content = """## ğŸ”¥ AI çƒ­ç‚¹ç®€æŠ¥

### ChatGPT-4
- **ç”¨é€”**: å¤šæ¨¡æ€ AI å¯¹è¯åŠ©æ‰‹
- **äº®ç‚¹**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€ä»£ç ç”Ÿæˆ
- **è¯„ä»·**: ç”¨æˆ·åé¦ˆç§¯æï¼Œè¡Œä¸šè®¤å¯åº¦é«˜

### Claude 3
- **ç”¨é€”**: ä¼ä¸šçº§ AI åŠ©æ‰‹
- **äº®ç‚¹**: å®‰å…¨æ€§å¼ºï¼Œé€‚åˆä¼ä¸šåº”ç”¨
- **è¯„ä»·**: åœ¨éšç§ä¿æŠ¤æ–¹é¢è¡¨ç°çªå‡º

**æ€»ç»“**: AI å·¥å…·æ­£åœ¨å‘å¤šæ¨¡æ€å’Œä¼ä¸šçº§åº”ç”¨æ–¹å‘å‘å±•

*æ³¨ï¼šè¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¯·è®¾ç½® DEEPSEEK_API_KEY è·å–çœŸå®ç»“æœ*"""
        elif "CV" in user_message or "è®¡ç®—æœºè§†è§‰" in user_message:
            mock_content = """## ğŸ¯ CV é¡¹ç›®/è¶‹åŠ¿åˆ†æ

### æŠ€æœ¯æ ˆ
- **æ¨¡å‹**: YOLOv8, ResNet-50
- **å¼•æ“**: TensorRT, ONNX Runtime
- **æ¡†æ¶**: PyTorch, TensorFlow
- **å…¶ä»–å·¥å…·**: OpenCV, CUDA

### è½åœ°åœºæ™¯
åŸºäºæœç´¢ç»“æœä¸­çš„å®é™…æ¡ˆä¾‹ï¼Œä¸»è¦åº”ç”¨äºï¼š
- è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹
- å·¥ä¸šè´¨æ£€ä¸­çš„ç¼ºé™·è¯†åˆ«
- åŒ»ç–—å½±åƒåˆ†æ

### æŠ€æœ¯ç‰¹ç‚¹
- å®æ—¶æ¨ç†æ€§èƒ½ä¼˜å¼‚
- æ”¯æŒè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- æ¨¡å‹å‹ç¼©æŠ€æœ¯æˆç†Ÿ

**æ•°æ®æ¥æº**: æ‰€æœ‰ä¿¡æ¯å‡åŸºäºæœç´¢ç»“æœï¼Œæ— è„‘è¡¥å†…å®¹

*æ³¨ï¼šè¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¯·è®¾ç½® DEEPSEEK_API_KEY è·å–çœŸå®ç»“æœ*"""
        else:
            mock_content = f"""è¿™æ˜¯æ¨¡æ‹Ÿ LLM å“åº”ã€‚

è¾“å…¥å†…å®¹æ‘˜è¦: {user_message[:100]}...

*æ³¨ï¼šè¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¯·è®¾ç½® DEEPSEEK_API_KEY è·å–çœŸå®ç»“æœ*"""
        
        # åˆ›å»ºä¸€ä¸ªç±»ä¼¼ ChatOpenAI å“åº”çš„å¯¹è±¡
        class MockResponse:
            def __init__(self, content: str):
                self.content = content
        
        return MockResponse(mock_content)


def get_llm(
    model: str = "deepseek-chat",
    temperature: float = 0.7,
    api_key: Optional[str] = None,
    use_mock: bool = False
) -> Any:
    """
    è·å– DeepSeek-V3 LLM å®ä¾‹æˆ–æ¨¡æ‹Ÿ LLM
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º deepseek-chatï¼ˆDeepSeek-V3ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
        api_key: API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        use_mock: å¦‚æœä¸º Trueï¼Œè¿”å›æ¨¡æ‹Ÿ LLMï¼ˆç”¨äºæµ‹è¯•ï¼‰
    
    Returns:
        ChatOpenAI å®ä¾‹æˆ– MockLLM å®ä¾‹
    """
    # #region agent log
    try:
        with open('/workspaces/social-media-assistant/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"sessionId":"debug-session","runId":"llm-check","hypothesisId":"B","location":"tools/llm_engine.py:45","message":"Checking DEEPSEEK_API_KEY","data":{"use_mock":use_mock},"timestamp":int(__import__('time').time()*1000)}) + '\n')
    except: pass
    # #endregion
    
    if use_mock:
        # #region agent log
        try:
            with open('/workspaces/social-media-assistant/.cursor/debug.log', 'a') as f:
                f.write(json.dumps({"sessionId":"debug-session","runId":"llm-check","hypothesisId":"B","location":"tools/llm_engine.py:50","message":"Using mock LLM","data":{"model":model},"timestamp":int(__import__('time').time()*1000)}) + '\n')
        except: pass
        # #endregion
        return MockLLM(model=model, temperature=temperature)
    
    api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
    
    # #region agent log
    try:
        with open('/workspaces/social-media-assistant/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({"sessionId":"debug-session","runId":"llm-check","hypothesisId":"B","location":"tools/llm_engine.py:60","message":"API key check result","data":{"has_key":bool(api_key)},"timestamp":int(__import__('time').time()*1000)}) + '\n')
    except: pass
    # #endregion
    
    if not api_key:
        raise ValueError(
            "DEEPSEEK_API_KEY æœªè®¾ç½®ã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEYï¼Œ"
            "æˆ–é€šè¿‡å‚æ•°ä¼ å…¥ api_keyï¼Œæˆ–ä½¿ç”¨ use_mock=True è¿›è¡Œæµ‹è¯•"
        )
    
    return ChatOpenAI(
        model=model,
        api_key=api_key,
        base_url="https://api.deepseek.com",
        temperature=temperature,
    )
