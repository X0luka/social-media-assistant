"""
Brief Agent - AI è¡Œä¸šçƒ­ç‚¹ç®€æŠ¥ç”Ÿæˆå™¨
æœç´¢ AI è¡Œä¸š 24h çƒ­ç‚¹ï¼Œæå–å·¥å…·åã€ç”¨é€”ã€è¯„ä»·ï¼Œè¾“å‡ºç¤¾äº¤åª’ä½“ç®€æŠ¥
"""
from core.state import AgentState
from tools.search import search_content
from tools.llm_engine import get_llm


def brief_generate_node(state: AgentState) -> AgentState:
    """
    ç”Ÿæˆ AI è¡Œä¸šçƒ­ç‚¹ç®€æŠ¥
    
    Args:
        state: AgentState çŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å« input_query
    
    Returns:
        æ›´æ–°åçš„ AgentStateï¼ŒåŒ…å«ç”Ÿæˆçš„ç®€æŠ¥å†…å®¹
    """
    input_query = state.get("input_query", "").strip()
    
    if not input_query:
        # å¦‚æœæ²¡æœ‰è¾“å…¥æŸ¥è¯¢ï¼Œä½¿ç”¨é»˜è®¤çš„ AI è¡Œä¸šçƒ­ç‚¹æœç´¢
        search_query = "AI industry news latest 24 hours tools"
    else:
        search_query = f"AI industry {input_query} latest 24 hours tools"
    
    try:
        # æœç´¢ AI è¡Œä¸š 24h çƒ­ç‚¹
        # å¦‚æœç¼ºå°‘ API keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        import os
        use_mock_search = not bool(os.getenv("TAVILY_API_KEY"))
        use_mock_llm = not bool(os.getenv("DEEPSEEK_API_KEY"))
        search_results = search_content(search_query, max_results=5, use_mock=use_mock_search)
        
        # è·å– LLM å®ä¾‹ï¼ˆå¦‚æœç¼ºå°‘ API keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿ LLMï¼‰
        llm = get_llm(temperature=0.7, use_mock=use_mock_llm)
        
        # æ„å»º System Prompt
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»æœç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç¤¾äº¤åª’ä½“ç®€æŠ¥ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ä»æœç´¢ç»“æœä¸­æå– AI å·¥å…·/äº§å“çš„åç§°
2. æ€»ç»“æ¯ä¸ªå·¥å…·çš„ç”¨é€”å’ŒåŠŸèƒ½
3. æå–ç”¨æˆ·è¯„ä»·æˆ–è¡Œä¸šåé¦ˆ
4. ç”Ÿæˆé€‚åˆç¤¾äº¤åª’ä½“å‘å¸ƒçš„ç®€æŠ¥ï¼ˆç®€æ´ã€æœ‰è¶£ã€ä¸“ä¸šï¼‰

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
## ğŸ”¥ AI çƒ­ç‚¹ç®€æŠ¥

### [å·¥å…·/äº§å“åç§° 1]
- **ç”¨é€”**: [ç®€è¦æè¿°]
- **äº®ç‚¹**: [å…³é”®ç‰¹æ€§æˆ–ä¼˜åŠ¿]
- **è¯„ä»·**: [ç”¨æˆ·åé¦ˆæˆ–è¡Œä¸šè§‚ç‚¹]

### [å·¥å…·/äº§å“åç§° 2]
...

**æ€»ç»“**: [ä¸€å¥è¯æ€»ç»“ä»Šæ—¥ AI è¡Œä¸šè¶‹åŠ¿]"""
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä»½ AI è¡Œä¸šçƒ­ç‚¹ç®€æŠ¥ï¼š

æœç´¢ç»“æœï¼š
{search_results}

è¯·ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚ï¼Œæå–å·¥å…·åã€ç”¨é€”ã€è¯„ä»·ï¼Œå¹¶ç”Ÿæˆç¤¾äº¤åª’ä½“ç®€æŠ¥ã€‚"""
        
        # è°ƒç”¨ LLM ç”Ÿæˆç®€æŠ¥
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = llm.invoke(messages)
        content = response.content if hasattr(response, 'content') else str(response)
        
        return {
            "content": content,
            "steps": [f"æ­¥éª¤: brief_generate - å·²ç”Ÿæˆ AI è¡Œä¸šçƒ­ç‚¹ç®€æŠ¥ï¼ˆæœç´¢: {search_query}ï¼‰"]
        }
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆç®€æŠ¥å¤±è´¥: {str(e)}"
        raise RuntimeError(f"æ­¥éª¤: brief_generate - {error_msg}") from e
