"""
CV Expert - è®¡ç®—æœºè§†è§‰é¡¹ç›®/è¶‹åŠ¿åˆ†æä¸“å®¶
æœç´¢ç‰¹å®š CV é¡¹ç›®/è¶‹åŠ¿ï¼Œä¸¥è°¨æå–æŠ€æœ¯æ ˆï¼Œåˆ†æè½åœ°åœºæ™¯ï¼Œç¦æ­¢è„‘è¡¥
"""
from core.state import AgentState
from tools.search import search_content
from tools.llm_engine import get_llm


def cv_generate_node(state: AgentState) -> AgentState:
    """
    ç”Ÿæˆ CV é¡¹ç›®/è¶‹åŠ¿åˆ†ææŠ¥å‘Š
    
    Args:
        state: AgentState çŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å« input_queryï¼ˆCV é¡¹ç›®/è¶‹åŠ¿å…³é”®è¯ï¼‰
    
    Returns:
        æ›´æ–°åçš„ AgentStateï¼ŒåŒ…å«ç”Ÿæˆçš„åˆ†ææŠ¥å‘Š
    """
    input_query = state.get("input_query", "").strip()
    
    if not input_query:
        raise ValueError("input_query ä¸èƒ½ä¸ºç©ºï¼Œè¯·æä¾› CV é¡¹ç›®æˆ–è¶‹åŠ¿å…³é”®è¯")
    
    try:
        # æœç´¢ç‰¹å®š CV é¡¹ç›®/è¶‹åŠ¿
        # å¦‚æœç¼ºå°‘ API keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        import os
        use_mock_search = not bool(os.getenv("TAVILY_API_KEY"))
        use_mock_llm = not bool(os.getenv("DEEPSEEK_API_KEY"))
        search_query = f"computer vision {input_query} project technology stack"
        search_results = search_content(search_query, max_results=5, use_mock=use_mock_search)
        
        # è·å– LLM å®ä¾‹ï¼ˆå¦‚æœç¼ºå°‘ API keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿ LLMï¼‰
        llm = get_llm(temperature=0.5, use_mock=use_mock_llm)  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥ç¡®ä¿ä¸¥è°¨æ€§
        
        # æ„å»º System Prompt
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„è®¡ç®—æœºè§†è§‰ä¸“å®¶ï¼Œæ“…é•¿ä»æœç´¢ç»“æœä¸­æå–æŠ€æœ¯ä¿¡æ¯å¹¶è¿›è¡Œåˆ†æã€‚

é‡è¦åŸåˆ™ï¼š
1. **ç¦æ­¢è„‘è¡¥**ï¼šæ‰€æœ‰ä¿¡æ¯å¿…é¡»åŸºäºæœç´¢ç»“æœï¼Œä¸å¾—æ·»åŠ æœç´¢ç»“æœä¸­æ²¡æœ‰çš„å†…å®¹
2. **ä¸¥è°¨æå–**ï¼šæŠ€æœ¯æ ˆå¿…é¡»å‡†ç¡®ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°ã€å¼•æ“ã€æ¡†æ¶ç­‰
3. **åŸºäºäº‹å®**ï¼šè½åœ°åœºæ™¯åˆ†æå¿…é¡»åŸºäºæœç´¢ç»“æœä¸­çš„å®é™…æ¡ˆä¾‹

ä½ çš„ä»»åŠ¡ï¼š
1. ä»æœç´¢ç»“æœä¸­ä¸¥è°¨æå–æŠ€æœ¯æ ˆï¼ˆæ¨¡å‹ã€å¼•æ“ã€æ¡†æ¶ï¼‰
2. åˆ†æé¡¹ç›®çš„è½åœ°åœºæ™¯ï¼ˆå¿…é¡»åŸºäºæœç´¢ç»“æœä¸­çš„å®é™…æ¡ˆä¾‹ï¼‰
3. æ€»ç»“æŠ€æœ¯ç‰¹ç‚¹å’Œåˆ›æ–°ç‚¹

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
## ğŸ¯ CV é¡¹ç›®/è¶‹åŠ¿åˆ†æ

### æŠ€æœ¯æ ˆ
- **æ¨¡å‹**: [ä»æœç´¢ç»“æœä¸­æå–çš„æ¨¡å‹åç§°]
- **å¼•æ“**: [ä»æœç´¢ç»“æœä¸­æå–çš„å¼•æ“åç§°]
- **æ¡†æ¶**: [ä»æœç´¢ç»“æœä¸­æå–çš„æ¡†æ¶åç§°]
- **å…¶ä»–å·¥å…·**: [å…¶ä»–ç›¸å…³æŠ€æœ¯]

### è½åœ°åœºæ™¯
[åŸºäºæœç´¢ç»“æœä¸­çš„å®é™…æ¡ˆä¾‹ï¼Œåˆ†æåº”ç”¨åœºæ™¯]

### æŠ€æœ¯ç‰¹ç‚¹
[åŸºäºæœç´¢ç»“æœæ€»ç»“çš„æŠ€æœ¯ç‰¹ç‚¹å’Œåˆ›æ–°ç‚¹]

**æ•°æ®æ¥æº**: æ‰€æœ‰ä¿¡æ¯å‡åŸºäºæœç´¢ç»“æœï¼Œæ— è„‘è¡¥å†…å®¹"""
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œå¯¹ CV é¡¹ç›®/è¶‹åŠ¿ '{input_query}' è¿›è¡Œä¸¥è°¨åˆ†æï¼š

æœç´¢ç»“æœï¼š
{search_results}

é‡è¦ï¼šè¯·ä¸¥æ ¼éµå®ˆ"ç¦æ­¢è„‘è¡¥"åŸåˆ™ï¼Œæ‰€æœ‰ä¿¡æ¯å¿…é¡»åŸºäºæœç´¢ç»“æœã€‚å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æ ‡æ³¨"æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚"""
        
        # è°ƒç”¨ LLM ç”Ÿæˆåˆ†ææŠ¥å‘Š
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = llm.invoke(messages)
        content = response.content if hasattr(response, 'content') else str(response)
        
        return {
            "content": content,
            "steps": [f"æ­¥éª¤: cv_generate - å·²ç”Ÿæˆ CV é¡¹ç›®åˆ†ææŠ¥å‘Šï¼ˆæŸ¥è¯¢: {input_query}ï¼‰"]
        }
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆ CV åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}"
        raise RuntimeError(f"æ­¥éª¤: cv_generate - {error_msg}") from e
